{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 4 : Análisis de opiniones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alberto Ramos Sánchez\n",
    "\n",
    "20/12/20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "En esta práctica se han comparado distintas técnicas de preprocesado NLP (básico, con *lemmatization* y con *stemming*), de selección de características (por frecuencia y *TF-IDF*) y de clasificación (*SVC* y *Random Forest*). Todas las técnicas aplicadas con el objetivo de clasificar opiniones.\n",
    "\n",
    "En la parte optativa se ha comparado cada técnica de preprocesado (básica, con *lemmatization* y  con *stemming*), utilizando un clasificador LSTM.\n",
    "\n",
    "Para la práctica se ha utilizado un dataset de opiniones de alimentos de Amazon que contiene más de 500000 muestras. Para cada opinión existen diferentes campos como identificador de producto, identificador de usuario y su nombre, calificación, resumen y texto de la opinión. Las puntuaciones en este conjunto de datos van de 1 a 5, siendo 1 muy mala y 5 muy buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alberto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos las opiniones y seleccionamos las columnas necesarias: *Text*, que contiene la opinión; y *Score*, donde está la puntuación dada al producto por la persona que escribió la opinión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  I have bought several of the Vitality canned d...\n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      4  This is a confection that has been around a fe...\n",
       "3      2  If you are looking for the secret ingredient i...\n",
       "4      5  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Reviews.csv')\n",
    "df.set_index('Id')\n",
    "df.drop(df.columns.difference(['Score','Text']), 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reducir la demora en la ejecución, se ha seleccionado una porción aleatoria del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = 10_000\n",
    "\n",
    "df = df.sample(n_rows)\n",
    "df.reset_index(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem = df.copy()\n",
    "df_stm = df.copy()\n",
    "\n",
    "reviews = df['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado básico de los datos\n",
    "\n",
    "###### Pasamos todas las letras a minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having tried a couple of other brands of gluten-free sandwich cookies, these are the best of the bunch.  they\\'re crunchy and true to the texture of the other \"real\" cookies that aren\\'t gluten-free.  some might think that the filling makes them a bit too sweet, but for me that just means i\\'ve satisfied my sweet tooth sooner!  the chocolate version from glutino is just as good and has a true \"chocolatey\" taste - something that isn\\'t there with the other gluten-free brands out there.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews.apply(lambda x: x.lower())\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminamos los signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'having tried a couple of other brands of glutenfree sandwich cookies these are the best of the bunch  theyre crunchy and true to the texture of the other real cookies that arent glutenfree  some might think that the filling makes them a bit too sweet but for me that just means ive satisfied my sweet tooth sooner  the chocolate version from glutino is just as good and has a true chocolatey taste  something that isnt there with the other glutenfree brands out there'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews.apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Eliminamos las *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews.apply(lambda x: nltk.word_tokenize(x))\n",
    "len(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "['tried', 'couple', 'brands', 'glutenfree', 'sandwich', 'cookies', 'best', 'bunch', 'theyre', 'crunchy', 'true', 'texture', 'real', 'cookies', 'arent', 'glutenfree', 'might', 'think', 'filling', 'makes', 'bit', 'sweet', 'means', 'ive', 'satisfied', 'sweet', 'tooth', 'sooner', 'chocolate', 'version', 'glutino', 'good', 'true', 'chocolatey', 'taste', 'something', 'isnt', 'glutenfree', 'brands']\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "reviews = reviews.apply(lambda x: [w for w in x if w not in stop_words])\n",
    "print(len(reviews[0]))\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tried couple brands glutenfree sandwich cookies best bunch theyre crunchy true texture real cookies arent glutenfree might think filling makes bit sweet means ive satisfied sweet tooth sooner chocolate version glutino good true chocolatey taste something isnt glutenfree brands'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = reviews.apply(lambda x: ' '.join(x))\n",
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de aplicar stemming es conseguir una misma representación para palabras con una misma raíz. Este método aplica un proceso heurístico para extraer la raíz de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tri coupl brand glutenfre sandwich cooki best bunch theyr crunchi true textur real cooki arent glutenfre might think fill make bit sweet mean ive satisfi sweet tooth sooner chocol version glutino good true chocolatey tast someth isnt glutenfre brand'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stem = stemmer.stem\n",
    "rewiews_stem = reviews.apply(lambda x: [stem(w) for w in x])\n",
    "\n",
    "df_stm['Text'] = rewiews_stem.apply(lambda x: ' '.join(x))\n",
    "df_stm.iloc[0]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método aplica la búsqueda en tablas para obtener la forma canónica o infinitivo (lemma) de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tried couple brand glutenfree sandwich cooky best bunch theyre crunchy true texture real cooky arent glutenfree might think filling make bit sweet mean ive satisfied sweet tooth sooner chocolate version glutino good true chocolatey taste something isnt glutenfree brand'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lem = lemmatizer.lemmatize\n",
    "reviews_lem = reviews.apply(lambda x: [lem(w) for w in x])\n",
    "\n",
    "df_lem['Text'] = reviews_lem.apply(lambda x: ' '.join(x))\n",
    "df_lem.iloc[0]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción de características es un proceso cuyo resultado es una representación en forma de matriz que indica la aparición de cada término en cada documentos del dataset (matriz término-documento).\n",
    "\n",
    "A continuación se muestra un ejemplo de extracción de características de frecuencias y *TF-IDF* para los datos preprocesados con *stemming*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset preprocesado con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_stem, df_test_stem = train_test_split(df_stm, train_size=0.7)\n",
    "df_train_stem, df_val_stem = train_test_split(df_train_stem, train_size=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5950x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 123973 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowTF = CountVectorizer(max_features=500, ngram_range=(1, 1))\n",
    "\n",
    "Xtrain_tf_stem = bowTF.fit_transform(df_train_stem['Text'])\n",
    "Xval_tf_stem = bowTF.transform(df_val_stem['Text'])\n",
    "Xtest_tf_stem = bowTF.transform(df_test_stem['Text'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Ytrain_tf_stem = encoder.fit_transform(df_train_stem['Score'])\n",
    "Yval_tf_stem = encoder.transform(df_val_stem['Score'])\n",
    "Ytest_tf_stem = encoder.transform(df_test_stem['Score'])\n",
    "\n",
    "Xtrain_tf_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5950x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 123973 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowTFIDF = TfidfVectorizer(max_features=500, ngram_range=(1, 1))\n",
    "\n",
    "Xtrain_tfidf_stem = bowTFIDF.fit_transform(df_train_stem['Text'])\n",
    "Xval_tfidf_stem = bowTFIDF.transform(df_val_stem['Text'])\n",
    "Xtest_tfidf_stem = bowTFIDF.transform(df_test_stem['Text'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Ytrain_tfidf_stem = encoder.fit_transform(df_train_stem['Score'])\n",
    "Yval_tfidf_stem = encoder.transform(df_val_stem['Score'])\n",
    "Ytest_tfidf_stem = encoder.transform(df_test_stem['Score'])\n",
    "\n",
    "Xtrain_tfidf_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra el mismo ejemplo para el dataset preprocesado con *lemmatization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_lem, df_test_lem = train_test_split(df_lem, train_size=0.7)\n",
    "df_train_lem, df_val_lem = train_test_split(df_train_lem, train_size=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5950x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 117865 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowTF = CountVectorizer(max_features=500, ngram_range=(1, 1))\n",
    "\n",
    "Xtrain_tf_lem = bowTF.fit_transform(df_train_lem['Text'])\n",
    "Xval_tf_lem = bowTF.transform(df_val_lem['Text'])\n",
    "Xtest_tf_lem = bowTF.transform(df_test_lem['Text'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Ytrain_tf_lem = encoder.fit_transform(df_train_lem['Score'])\n",
    "Yval_tf_lem = encoder.transform(df_val_lem['Score'])\n",
    "Ytest_tf_lem = encoder.transform(df_test_lem['Score'])\n",
    "\n",
    "Xtrain_tf_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5950x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 117865 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowTFIDF = TfidfVectorizer(max_features=500, ngram_range=(1, 1))\n",
    "\n",
    "Xtrain_tfidf_lem = bowTFIDF.fit_transform(df_train_lem['Text'])\n",
    "Xval_tfidf_lem = bowTFIDF.transform(df_val_lem['Text'])\n",
    "Xtest_tfidf_lem = bowTFIDF.transform(df_test_lem['Text'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Ytrain_tfidf_lem = encoder.fit_transform(df_train_lem['Score'])\n",
    "Yval_tfidf_lem = encoder.transform(df_val_lem['Score'])\n",
    "Ytest_tfidf_lem = encoder.transform(df_test_lem['Score'])\n",
    "\n",
    "Xtrain_tfidf_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original\n",
    "\n",
    "A continuación se muestra el mismo ejemplo para el dataset con un preprocesado básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_orig, df_test_orig = train_test_split(df, train_size=0.7)\n",
    "df_train_orig, df_val_orig = train_test_split(df_train_orig, train_size=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5950x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 113525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowTF = CountVectorizer(max_features=500, ngram_range=(1, 1))\n",
    "\n",
    "Xtrain_tf_orig = bowTF.fit_transform(df_train_orig['Text'])\n",
    "Xval_tf_orig = bowTF.transform(df_val_orig['Text'])\n",
    "Xtest_tf_orig = bowTF.transform(df_test_orig['Text'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Ytrain_tf_orig = encoder.fit_transform(df_train_orig['Score'])\n",
    "Yval_tf_orig = encoder.transform(df_val_orig['Score'])\n",
    "Ytest_tf_orig = encoder.transform(df_test_orig['Score'])\n",
    "\n",
    "Xtrain_tf_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5950x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 113525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowTFIDF = TfidfVectorizer(max_features=500, ngram_range=(1, 1))\n",
    "\n",
    "Xtrain_tfidf_orig = bowTFIDF.fit_transform(df_train_orig['Text'])\n",
    "Xval_tfidf_orig = bowTFIDF.transform(df_val_orig['Text'])\n",
    "Xtest_tfidf_orig = bowTFIDF.transform(df_test_orig['Text'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Ytrain_tfidf_orig = encoder.fit_transform(df_train_orig['Score'])\n",
    "Yval_tfidf_orig = encoder.transform(df_val_orig['Score'])\n",
    "Ytest_tfidf_orig = encoder.transform(df_test_orig['Score'])\n",
    "\n",
    "Xtrain_tfidf_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción caracteristicas (genérico)\n",
    "\n",
    "Se ha implementando la función *feature_extraction* con el objetivo de comparar los distintos tipos de preprocesados según el rango de *n-gramas* y el número de características seleccionadas. En el siguiente apartado contrastaremos para distintos parámetros de esta función a los clasificadores *SVC* y *Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset, vectorizer, max_features, ngram_range, test_size=0.3, val_size=0.15):\n",
    "    data_train, data_test = train_test_split(dataset, train_size=1-test_size)\n",
    "    data_train, data_val = train_test_split(data_train, train_size=1-val_size)\n",
    "    \n",
    "    bow = vectorizer(max_features=max_features, ngram_range=ngram_range)\n",
    "    \n",
    "    Xtrain = bow.fit_transform(data_train['Text'])\n",
    "    Xval = bow.transform(data_val['Text'])\n",
    "    Xtest = bow.transform(data_test['Text'])\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    Ytrain = encoder.fit_transform(data_train['Score'])\n",
    "    Yval = encoder.transform(data_val['Score'])\n",
    "    Ytest = encoder.transform(data_test['Score'])\n",
    "    \n",
    "    return Xtrain, Ytrain, Xval, Yval, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n",
    "\n",
    "Buscaremos para cada clasificador el ajuste de parámetros y el selector de características que da mejores resultados entre los propuestos a continuación. La métrica de comparación será la precisión (*accuracy*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [500, 1000, 2000] # Número de características a seleccionar\n",
    "ngram_range = [(1, 1), (1, 2), (1, 3)] # Ngramas\n",
    "\n",
    "vect = [CountVectorizer, TfidfVectorizer] # Selectores de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, Xtrain, Ytrain, Xval, Yval):\n",
    "    classifier.fit(Xtrain, Ytrain)\n",
    "    Ypredicted = classifier.predict(Xval)\n",
    "    \n",
    "    accuracy = accuracy_score(Yval, Ypredicted)*100\n",
    "    precision = precision_score(Yval, Ypredicted, average='macro', zero_division=1)*100\n",
    "    recall = recall_score(Yval, Ypredicted, average='macro', zero_division=1)*100\n",
    "    \n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor ajuste de parámetros para la clasificación con SVC de los datos preprocesados con *stemming* es:\n",
    "- Ngramas (1, 3). Nº de características: 500\n",
    "- Vectorizer: TfidfVectorizer\n",
    "- Accuracy: 67.71\n",
    "- Precision: 81.44\n",
    "- Recall: 24.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 59.90\n",
      "\t\tPrecision: 79.19\n",
      "\t\tRecall: 22.31\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 89.00\n",
      "\t\tRecall: 24.64\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 63.52\n",
      "\t\tPrecision: 69.91\n",
      "\t\tRecall: 22.06\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.38\n",
      "\t\tPrecision: 62.93\n",
      "\t\tRecall: 21.94\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.48\n",
      "\t\tPrecision: 76.76\n",
      "\t\tRecall: 23.70\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.05\n",
      "\t\tPrecision: 82.51\n",
      "\t\tRecall: 23.49\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.10\n",
      "\t\tPrecision: 79.06\n",
      "\t\tRecall: 22.86\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.14\n",
      "\t\tPrecision: 82.76\n",
      "\t\tRecall: 24.06\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 88.22\n",
      "\t\tRecall: 24.30\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.14\n",
      "\t\tPrecision: 70.92\n",
      "\t\tRecall: 24.75\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.95\n",
      "\t\tPrecision: 63.86\n",
      "\t\tRecall: 25.53\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.71\n",
      "\t\tPrecision: 81.44\n",
      "\t\tRecall: 24.34\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 64.86\n",
      "\t\tPrecision: 67.80\n",
      "\t\tRecall: 25.37\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 62.95\n",
      "\t\tPrecision: 64.85\n",
      "\t\tRecall: 24.95\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 68.95\n",
      "\t\tRecall: 26.46\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.24\n",
      "\t\tPrecision: 53.77\n",
      "\t\tRecall: 25.56\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.57\n",
      "\t\tPrecision: 81.75\n",
      "\t\tRecall: 25.92\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.57\n",
      "\t\tPrecision: 76.85\n",
      "\t\tRecall: 25.13\n",
      "Best solution: \n",
      "\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.71\n",
      "\t\tPrecision: 81.44\n",
      "\t\tRecall: 24.34\n"
     ]
    }
   ],
   "source": [
    "bestData = [None, None, None, None, None, None] # split of best solution\n",
    "best_features = [None, None]\n",
    "stats = [0, 0, 0] # accuracy, precision, recall of best solution\n",
    "best_svc = None # best svc\n",
    "best_vectorizer = None\n",
    "\n",
    "for vec in vect:\n",
    "    for ft in features:\n",
    "        for ngram in ngram_range:\n",
    "            print(\"Feature extraction: ngram {0}, n_feat {1}\".format(ngram, ft))\n",
    "            \n",
    "            Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df_stm, vec, ft, ngram)\n",
    "            \n",
    "            svc = SVC()\n",
    "            \n",
    "            acc, prec, rec = train_classifier(svc, Xtrain, Ytrain, Xval, Yval)\n",
    "            \n",
    "            \n",
    "            print(\"\\tStats: \")\n",
    "            print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "            print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "            print(\"\\t\\tRecall: {0:.2f}\".format(rec))\n",
    "            \n",
    "            if acc > stats[0]:\n",
    "                bestData = [Xtrain, Ytrain, Xval, Yval, Xtest, Ytest]\n",
    "                stats = [acc, prec, rec]\n",
    "                best_svc = svc\n",
    "                best_features = [ft, ngram]\n",
    "                best_vectorizer = vec\n",
    "\n",
    "print(\"Best solution: \\n\")\n",
    "print(\"Feature extraction: ngram {0}, n_feat {1}\".format(best_features[1], best_features[0]))\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(best_vectorizer.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(stats[0]))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(stats[1]))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor ajuste de parámetros para la clasificación con SVC de los datos preprocesados con *lemmatization* es:\n",
    "- Ngramas (1, 2). Nº de características: 1000\n",
    "- Vectorizer: TfidfVectorizer\n",
    "- Accuracy: 67.81\n",
    "- Precision: 88.06\n",
    "- Recall: 23.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.43\n",
      "\t\tPrecision: 91.34\n",
      "\t\tRecall: 22.98\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 63.52\n",
      "\t\tPrecision: 89.55\n",
      "\t\tRecall: 22.93\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.57\n",
      "\t\tPrecision: 79.56\n",
      "\t\tRecall: 23.80\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.52\n",
      "\t\tPrecision: 86.91\n",
      "\t\tRecall: 22.83\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.81\n",
      "\t\tPrecision: 88.06\n",
      "\t\tRecall: 23.94\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.48\n",
      "\t\tPrecision: 87.39\n",
      "\t\tRecall: 22.73\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.62\n",
      "\t\tPrecision: 86.37\n",
      "\t\tRecall: 23.16\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 63.90\n",
      "\t\tPrecision: 88.71\n",
      "\t\tRecall: 23.12\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.29\n",
      "\t\tPrecision: 87.80\n",
      "\t\tRecall: 22.30\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.00\n",
      "\t\tPrecision: 60.75\n",
      "\t\tRecall: 23.58\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.24\n",
      "\t\tPrecision: 76.58\n",
      "\t\tRecall: 25.20\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.76\n",
      "\t\tPrecision: 78.96\n",
      "\t\tRecall: 24.81\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.76\n",
      "\t\tPrecision: 68.77\n",
      "\t\tRecall: 25.76\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.67\n",
      "\t\tPrecision: 74.47\n",
      "\t\tRecall: 27.33\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 64.86\n",
      "\t\tPrecision: 61.13\n",
      "\t\tRecall: 25.27\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.76\n",
      "\t\tPrecision: 75.28\n",
      "\t\tRecall: 24.23\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.14\n",
      "\t\tPrecision: 81.50\n",
      "\t\tRecall: 26.76\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.14\n",
      "\t\tPrecision: 81.32\n",
      "\t\tRecall: 24.17\n",
      "Best solution: \n",
      "\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.81\n",
      "\t\tPrecision: 88.06\n",
      "\t\tRecall: 23.94\n"
     ]
    }
   ],
   "source": [
    "bestData = [None, None, None, None, None, None] # split of best solution\n",
    "best_features = [None, None]\n",
    "stats = [0, 0, 0] # accuracy, precision, recall of best solution\n",
    "best_svc = None # best svc\n",
    "best_vectorizer = None\n",
    "\n",
    "for vec in vect:\n",
    "    for ft in features:\n",
    "        for ngram in ngram_range:\n",
    "            print(\"Feature extraction: ngram {0}, n_feat {1}\".format(ngram, ft))\n",
    "            \n",
    "            Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df_lem, vec, ft, ngram)\n",
    "            \n",
    "            svc = SVC()\n",
    "            \n",
    "            acc, prec, rec = train_classifier(svc, Xtrain, Ytrain, Xval, Yval)\n",
    "            \n",
    "            \n",
    "            print(\"\\tStats: \")\n",
    "            print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "            print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "            print(\"\\t\\tRecall: {0:.2f}\".format(rec))\n",
    "            \n",
    "            if acc > stats[0]:\n",
    "                bestData = [Xtrain, Ytrain, Xval, Yval, Xtest, Ytest]\n",
    "                stats = [acc, prec, rec]\n",
    "                best_svc = svc\n",
    "                best_features = [ft, ngram]\n",
    "                best_vectorizer = vec\n",
    "\n",
    "print(\"Best solution: \\n\")\n",
    "print(\"Feature extraction: ngram {0}, n_feat {1}\".format(best_features[1], best_features[0]))\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(best_vectorizer.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(stats[0]))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(stats[1]))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor ajuste de parámetros para la clasificación con SVC de los datos con un preprocesado básico es:\n",
    "- Ngramas (1, 1). Nº de características: 500\n",
    "- Vectorizer: TfidfVectorizer\n",
    "- Accuracy: 70.10\n",
    "- Precision: 67.77\n",
    "- Recall: 25.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.52\n",
      "\t\tPrecision: 86.40\n",
      "\t\tRecall: 23.19\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.67\n",
      "\t\tPrecision: 92.85\n",
      "\t\tRecall: 22.46\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 63.43\n",
      "\t\tPrecision: 90.41\n",
      "\t\tRecall: 22.10\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.57\n",
      "\t\tPrecision: 87.28\n",
      "\t\tRecall: 24.37\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.90\n",
      "\t\tPrecision: 89.88\n",
      "\t\tRecall: 23.01\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.57\n",
      "\t\tPrecision: 88.97\n",
      "\t\tRecall: 22.89\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.62\n",
      "\t\tPrecision: 89.31\n",
      "\t\tRecall: 23.48\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.48\n",
      "\t\tPrecision: 91.01\n",
      "\t\tRecall: 22.29\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.05\n",
      "\t\tPrecision: 78.99\n",
      "\t\tRecall: 22.03\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 70.10\n",
      "\t\tPrecision: 67.77\n",
      "\t\tRecall: 25.37\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 68.57\n",
      "\t\tPrecision: 77.69\n",
      "\t\tRecall: 25.86\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 74.17\n",
      "\t\tRecall: 24.45\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.14\n",
      "\t\tPrecision: 78.92\n",
      "\t\tRecall: 27.79\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 64.10\n",
      "\t\tPrecision: 64.29\n",
      "\t\tRecall: 24.42\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 64.67\n",
      "\t\tPrecision: 71.02\n",
      "\t\tRecall: 23.66\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.10\n",
      "\t\tPrecision: 79.57\n",
      "\t\tRecall: 24.89\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 69.43\n",
      "\t\tPrecision: 74.86\n",
      "\t\tRecall: 26.60\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 69.05\n",
      "\t\tPrecision: 83.10\n",
      "\t\tRecall: 26.55\n",
      "Best solution: \n",
      "\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 70.10\n",
      "\t\tPrecision: 67.77\n",
      "\t\tRecall: 25.37\n"
     ]
    }
   ],
   "source": [
    "bestData = [None, None, None, None, None, None] # split of best solution\n",
    "best_features = [None, None]\n",
    "stats = [0, 0, 0] # accuracy, precision, recall of best solution\n",
    "best_svc = None # best svc\n",
    "best_vectorizer = None\n",
    "\n",
    "for vec in vect:\n",
    "    for ft in features:\n",
    "        for ngram in ngram_range:\n",
    "            print(\"Feature extraction: ngram {0}, n_feat {1}\".format(ngram, ft))\n",
    "            \n",
    "            Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df, vec, ft, ngram)\n",
    "            \n",
    "            svc = SVC()\n",
    "            \n",
    "            acc, prec, rec = train_classifier(svc, Xtrain, Ytrain, Xval, Yval)\n",
    "            \n",
    "            \n",
    "            print(\"\\tStats: \")\n",
    "            print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "            print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "            print(\"\\t\\tRecall: {0:.2f}\".format(rec))\n",
    "            \n",
    "            if acc > stats[0]:\n",
    "                bestData = [Xtrain, Ytrain, Xval, Yval, Xtest, Ytest]\n",
    "                stats = [acc, prec, rec]\n",
    "                best_svc = svc\n",
    "                best_features = [ft, ngram]\n",
    "                best_vectorizer = vec\n",
    "\n",
    "print(\"Best solution: \\n\")\n",
    "print(\"Feature extraction: ngram {0}, n_feat {1}\".format(best_features[1], best_features[0]))\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(best_vectorizer.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(stats[0]))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(stats[1]))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor ajuste de parámetros para la clasificación con *Random Forest* de los datos preprocesados con *stemming* es:\n",
    "\n",
    "- Ngramas (1, 2). Nº de características: 500\n",
    "- Vectorizer: TfidfVectorizer\n",
    "- Accuracy: 69.52\n",
    "- Precision: 63.44\n",
    "- Recall: 26.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.81\n",
      "\t\tPrecision: 40.58\n",
      "\t\tRecall: 23.80\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.90\n",
      "\t\tPrecision: 66.50\n",
      "\t\tRecall: 26.16\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.05\n",
      "\t\tPrecision: 53.57\n",
      "\t\tRecall: 26.10\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.62\n",
      "\t\tPrecision: 64.82\n",
      "\t\tRecall: 26.31\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.57\n",
      "\t\tPrecision: 66.25\n",
      "\t\tRecall: 26.86\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.48\n",
      "\t\tPrecision: 60.44\n",
      "\t\tRecall: 25.80\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.24\n",
      "\t\tPrecision: 60.45\n",
      "\t\tRecall: 24.83\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.76\n",
      "\t\tPrecision: 55.31\n",
      "\t\tRecall: 27.80\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.62\n",
      "\t\tPrecision: 73.25\n",
      "\t\tRecall: 26.23\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.33\n",
      "\t\tPrecision: 63.04\n",
      "\t\tRecall: 25.35\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 69.52\n",
      "\t\tPrecision: 63.44\n",
      "\t\tRecall: 26.15\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.48\n",
      "\t\tPrecision: 63.70\n",
      "\t\tRecall: 25.61\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 52.67\n",
      "\t\tRecall: 26.25\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 69.05\n",
      "\t\tPrecision: 72.90\n",
      "\t\tRecall: 27.18\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.43\n",
      "\t\tPrecision: 76.97\n",
      "\t\tRecall: 27.24\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.14\n",
      "\t\tPrecision: 83.58\n",
      "\t\tRecall: 25.57\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.00\n",
      "\t\tPrecision: 78.68\n",
      "\t\tRecall: 26.41\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.52\n",
      "\t\tPrecision: 75.36\n",
      "\t\tRecall: 25.19\n",
      "Best solution: \n",
      "\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 69.52\n",
      "\t\tPrecision: 63.44\n",
      "\t\tRecall: 26.15\n"
     ]
    }
   ],
   "source": [
    "bestData = [None, None, None, None, None, None] # split of best solution\n",
    "best_features = [None, None]\n",
    "stats = [0, 0, 0] # accuracy, precision, recall of best solution\n",
    "best_rf = None # best svc\n",
    "best_vectorizer = None\n",
    "\n",
    "for vec in vect:\n",
    "    for ft in features:\n",
    "        for ngram in ngram_range:\n",
    "            print(\"Feature extraction: ngram {0}, n_feat {1}\".format(ngram, ft))\n",
    "            \n",
    "            Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df_stm, vec, ft, ngram)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            \n",
    "            acc, prec, rec = train_classifier(rf, Xtrain, Ytrain, Xval, Yval)\n",
    "            \n",
    "            \n",
    "            print(\"\\tStats: \")\n",
    "            print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "            print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "            print(\"\\t\\tRecall: {0:.2f}\".format(rec))\n",
    "            \n",
    "            if acc > stats[0]:\n",
    "                bestData = [Xtrain, Ytrain, Xval, Yval, Xtest, Ytest]\n",
    "                stats = [acc, prec, rec]\n",
    "                best_rf = rf\n",
    "                best_features = [ft, ngram]\n",
    "                best_vectorizer = vec\n",
    "\n",
    "print(\"Best solution: \\n\")\n",
    "print(\"Feature extraction: ngram {0}, n_feat {1}\".format(best_features[1], best_features[0]))\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(best_vectorizer.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(stats[0]))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(stats[1]))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor ajuste de parámetros para la clasificación con *Random Forest* de los datos preprocesados con *lemmatization* es:\n",
    "- Ngramas (1, 3). Nº de características: 1000\n",
    "- Vectorizer: TfidfVectorizer\n",
    "- Accuracy: 67.90\n",
    "- Precision: 71.96\n",
    "- Recall: 25.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.19\n",
      "\t\tPrecision: 71.25\n",
      "\t\tRecall: 24.35\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.52\n",
      "\t\tPrecision: 67.60\n",
      "\t\tRecall: 26.20\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.81\n",
      "\t\tPrecision: 63.86\n",
      "\t\tRecall: 26.21\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.24\n",
      "\t\tPrecision: 60.03\n",
      "\t\tRecall: 25.60\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 62.86\n",
      "\t\tPrecision: 74.37\n",
      "\t\tRecall: 25.36\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.33\n",
      "\t\tPrecision: 46.78\n",
      "\t\tRecall: 24.58\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.95\n",
      "\t\tPrecision: 64.48\n",
      "\t\tRecall: 25.78\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.05\n",
      "\t\tPrecision: 57.79\n",
      "\t\tRecall: 25.35\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.33\n",
      "\t\tPrecision: 67.64\n",
      "\t\tRecall: 25.75\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 64.57\n",
      "\t\tPrecision: 67.16\n",
      "\t\tRecall: 23.62\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.67\n",
      "\t\tPrecision: 58.32\n",
      "\t\tRecall: 23.44\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 63.14\n",
      "\t\tPrecision: 65.35\n",
      "\t\tRecall: 24.24\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.67\n",
      "\t\tPrecision: 68.70\n",
      "\t\tRecall: 26.44\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.24\n",
      "\t\tPrecision: 59.70\n",
      "\t\tRecall: 25.57\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.90\n",
      "\t\tPrecision: 71.96\n",
      "\t\tRecall: 25.36\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.52\n",
      "\t\tPrecision: 83.82\n",
      "\t\tRecall: 26.55\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.71\n",
      "\t\tPrecision: 73.79\n",
      "\t\tRecall: 25.66\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.76\n",
      "\t\tPrecision: 75.87\n",
      "\t\tRecall: 26.23\n",
      "Best solution: \n",
      "\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.90\n",
      "\t\tPrecision: 71.96\n",
      "\t\tRecall: 25.36\n"
     ]
    }
   ],
   "source": [
    "bestData = [None, None, None, None, None, None] # split of best solution\n",
    "best_features = [None, None]\n",
    "stats = [0, 0, 0] # accuracy, precision, recall of best solution\n",
    "best_rf = None # best svc\n",
    "best_vectorizer = None\n",
    "\n",
    "for vec in vect:\n",
    "    for ft in features:\n",
    "        for ngram in ngram_range:\n",
    "            print(\"Feature extraction: ngram {0}, n_feat {1}\".format(ngram, ft))\n",
    "            \n",
    "            Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df_lem, vec, ft, ngram)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            \n",
    "            acc, prec, rec = train_classifier(rf, Xtrain, Ytrain, Xval, Yval)\n",
    "            \n",
    "            \n",
    "            print(\"\\tStats: \")\n",
    "            print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "            print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "            print(\"\\t\\tRecall: {0:.2f}\".format(rec))\n",
    "            \n",
    "            if acc > stats[0]:\n",
    "                bestData = [Xtrain, Ytrain, Xval, Yval, Xtest, Ytest]\n",
    "                stats = [acc, prec, rec]\n",
    "                best_rf = rf\n",
    "                best_features = [ft, ngram]\n",
    "                best_vectorizer = vec\n",
    "\n",
    "print(\"Best solution: \\n\")\n",
    "print(\"Feature extraction: ngram {0}, n_feat {1}\".format(best_features[1], best_features[0]))\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(best_vectorizer.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(stats[0]))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(stats[1]))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor ajuste de parámetros para la clasificación con *Random Forest* de los datos con un preprocesado básico es:\n",
    "- Ngramas (1, 1). Nº de características: 1000\n",
    "- Vectorizer: CountVectorizer\n",
    "- Accuracy: 68.29\n",
    "- Precision: 76.50\n",
    "- Recall: 24.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.38\n",
      "\t\tPrecision: 58.13\n",
      "\t\tRecall: 25.58\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.48\n",
      "\t\tPrecision: 58.26\n",
      "\t\tRecall: 24.78\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 67.33\n",
      "\t\tPrecision: 66.82\n",
      "\t\tRecall: 26.08\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 68.29\n",
      "\t\tPrecision: 76.50\n",
      "\t\tRecall: 24.84\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 62.48\n",
      "\t\tPrecision: 59.91\n",
      "\t\tRecall: 24.90\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.95\n",
      "\t\tPrecision: 68.01\n",
      "\t\tRecall: 27.35\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 65.14\n",
      "\t\tPrecision: 75.16\n",
      "\t\tRecall: 27.49\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 64.67\n",
      "\t\tPrecision: 77.44\n",
      "\t\tRecall: 26.99\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 75.21\n",
      "\t\tRecall: 27.26\n",
      "Feature extraction: ngram (1, 1), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.05\n",
      "\t\tPrecision: 67.56\n",
      "\t\tRecall: 25.31\n",
      "Feature extraction: ngram (1, 2), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.24\n",
      "\t\tPrecision: 45.86\n",
      "\t\tRecall: 24.44\n",
      "Feature extraction: ngram (1, 3), n_feat 500\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.19\n",
      "\t\tPrecision: 57.51\n",
      "\t\tRecall: 26.24\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 67.62\n",
      "\t\tPrecision: 58.58\n",
      "\t\tRecall: 25.14\n",
      "Feature extraction: ngram (1, 2), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.19\n",
      "\t\tPrecision: 76.53\n",
      "\t\tRecall: 23.45\n",
      "Feature extraction: ngram (1, 3), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.00\n",
      "\t\tPrecision: 60.85\n",
      "\t\tRecall: 27.22\n",
      "Feature extraction: ngram (1, 1), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 65.14\n",
      "\t\tPrecision: 77.20\n",
      "\t\tRecall: 25.39\n",
      "Feature extraction: ngram (1, 2), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 66.38\n",
      "\t\tPrecision: 69.27\n",
      "\t\tRecall: 27.02\n",
      "Feature extraction: ngram (1, 3), n_feat 2000\n",
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 68.10\n",
      "\t\tPrecision: 70.72\n",
      "\t\tRecall: 25.42\n",
      "Best solution: \n",
      "\n",
      "Feature extraction: ngram (1, 1), n_feat 1000\n",
      "\tStats: \n",
      "\t\tVectorizer: CountVectorizer\n",
      "\t\tAccuracy: 68.29\n",
      "\t\tPrecision: 76.50\n",
      "\t\tRecall: 24.84\n"
     ]
    }
   ],
   "source": [
    "bestData = [None, None, None, None, None, None] # split of best solution\n",
    "best_features = [None, None]\n",
    "stats = [0, 0, 0] # accuracy, precision, recall of best solution\n",
    "best_rf = None # best svc\n",
    "best_vectorizer = None\n",
    "\n",
    "for vec in vect:\n",
    "    for ft in features:\n",
    "        for ngram in ngram_range:\n",
    "            print(\"Feature extraction: ngram {0}, n_feat {1}\".format(ngram, ft))\n",
    "            \n",
    "            Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df, vec, ft, ngram)\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            \n",
    "            acc, prec, rec = train_classifier(rf, Xtrain, Ytrain, Xval, Yval)\n",
    "            \n",
    "            \n",
    "            print(\"\\tStats: \")\n",
    "            print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "            print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "            print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "            print(\"\\t\\tRecall: {0:.2f}\".format(rec))\n",
    "            \n",
    "            if acc > stats[0]:\n",
    "                bestData = [Xtrain, Ytrain, Xval, Yval, Xtest, Ytest]\n",
    "                stats = [acc, prec, rec]\n",
    "                best_rf = rf\n",
    "                best_features = [ft, ngram]\n",
    "                best_vectorizer = vec\n",
    "\n",
    "print(\"Best solution: \\n\")\n",
    "print(\"Feature extraction: ngram {0}, n_feat {1}\".format(best_features[1], best_features[0]))\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(best_vectorizer.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(stats[0]))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(stats[1]))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(stats[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En todos los resultados obtenemos un valor de recall bajo y una precisión mayor, lo que indica que el clasificador es muy estricto y comete muchos falsos positivos. Es posible que se obtenga este resultado por el bajo número de muestras utilizadas o porque las muestras estén desbalanceadas (en el apartado posterior se comprueba con un dataset balanceado).\n",
    "\n",
    "En general, para ambos algoritmos, cuando se aplica preprocesado (*stemming* o *lemmatization*) es necesario seleccionar menos características para alcanzar una misma exactitud que sin aplicar estos preprocesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se comprueba con un dataset balanceado sobre los mejores parámetros obtenidos.\n",
    "\n",
    "Equilibramos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Text\n",
      "Score       \n",
      "1      10000\n",
      "2      10000\n",
      "3      10000\n",
      "4      10000\n",
      "5      10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>We purchased a six box carton of the product. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We ordered Wolfgang Puck Sumatra blend and Col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>These were selling in my gym and I decided to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This product has to be one of the most frustra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Zero out of five cats, all ferals, said euck.&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a great drink that has the flavor of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5</td>\n",
       "      <td>Mad Dog's Revenge was an awesome purchase!! I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5</td>\n",
       "      <td>I really like these. One reviewer mentioned th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5</td>\n",
       "      <td>I like this tea better than any I can find at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5</td>\n",
       "      <td>I did research on the benefits of coconut oil ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score                                               Text\n",
       "0          1  We purchased a six box carton of the product. ...\n",
       "1          1  We ordered Wolfgang Puck Sumatra blend and Col...\n",
       "2          1  These were selling in my gym and I decided to ...\n",
       "3          1  This product has to be one of the most frustra...\n",
       "4          1  Zero out of five cats, all ferals, said euck.<...\n",
       "...      ...                                                ...\n",
       "49995      5  This is a great drink that has the flavor of m...\n",
       "49996      5  Mad Dog's Revenge was an awesome purchase!! I ...\n",
       "49997      5  I really like these. One reviewer mentioned th...\n",
       "49998      5  I like this tea better than any I can find at ...\n",
       "49999      5  I did research on the benefits of coconut oil ...\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eq = pd.read_csv('Reviews.csv')\n",
    "df_eq.set_index('Id')\n",
    "df_eq.drop(df_eq.columns.difference(['Score','Text']), 1, inplace=True)\n",
    "\n",
    "n_rows_each_score = 50_000//5\n",
    "\n",
    "df_sampled = []\n",
    "for score_i in range(1, 5+1, 1):\n",
    "    df_sampled.append(df_eq[df_eq['Score'] == score_i].sample(n_rows_each_score))\n",
    "df_eq = pd.concat(df_sampled, ignore_index=True)\n",
    "\n",
    "print(df_eq.groupby(['Score']).count())\n",
    "df_eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_eq['Text']\n",
    "\n",
    "reviews = reviews.apply(lambda x: x.lower())\n",
    "\n",
    "reviews = reviews.apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "\n",
    "reviews = reviews.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "reviews = reviews.apply(lambda x: [w for w in x if w not in stop_words])\n",
    "\n",
    "df_eq['Text'] = reviews.apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tStats: \n",
      "\t\tVectorizer: TfidfVectorizer\n",
      "\t\tAccuracy: 50.48\n",
      "\t\tPrecision: 50.25\n",
      "\t\tRecall: 50.44\n"
     ]
    }
   ],
   "source": [
    "ft = 500\n",
    "ngram = (1, 1)\n",
    "vec = TfidfVectorizer\n",
    "\n",
    "Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = \\\n",
    "                feature_extraction(df_eq, vec, ft, ngram)\n",
    "            \n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "acc, prec, rec = train_classifier(rf, Xtrain, Ytrain, Xval, Yval)\n",
    "\n",
    "\n",
    "print(\"\\tStats: \")\n",
    "print(\"\\t\\tVectorizer: {0}\".format(vec.__name__))\n",
    "print(\"\\t\\tAccuracy: {0:.2f}\".format(acc))\n",
    "print(\"\\t\\tPrecision: {0:.2f}\".format(prec))\n",
    "print(\"\\t\\tRecall: {0:.2f}\".format(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se consigue aumentar el *recall* siendo este equivalente a la precisión. Sin embargo, es mucho menor. Posiblemente, si aumentáramos el número de muestras que seleccionamos posiblemente conseguiríamos mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el apartado opcional compararemos la arquitectura *LSTM* para los distintos tipos de preprocesados. Para el ajuste de hiperparámetros utilizaremos el módulo de optimización de hiperparámetros *talos*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos\n",
    "from talos.model.normalizers import lr_normalizer\n",
    "\n",
    "from talos import Evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout, Input, Embedding\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función *modelo* crea, compila y entrena el modelo de la *LSTM* según los parámetros aportados por *talos* en el diccionario *params*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo(Xtrain, Ytrain, Xval, Yval, params):\n",
    "    inputs = Input(name='inputs', shape=[max_len])\n",
    "    \n",
    "    layer = Embedding(max_words, params['emb_dim'], input_length=max_len)(inputs)\n",
    "    \n",
    "    layer = LSTM(params['lstm_dim'])(layer)\n",
    "    \n",
    "    layer = Dense(params['fc1_dim'], name='FC1')(layer)\n",
    "    \n",
    "    layer = Activation('relu')(layer)\n",
    "    \n",
    "    layer = Dropout(params['dropout'])(layer)\n",
    "    \n",
    "    layer = Dense(5, name='output_layer')(layer)\n",
    "    \n",
    "    layer = Activation('softmax')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    \n",
    "    model.compile(loss=params['losses'],\n",
    "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(Xtrain, Ytrain,\n",
    "                        validation_data=[Xval, Yval],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10)],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente diccionario se definen los hiperparámetros de la red *LSTM* que vamos a optimizar con *talos*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"emb_dim\": [25, 50],\n",
    "    \"lstm_dim\": [32, 64, 128],\n",
    "    \"fc1_dim\": [128, 256, 512],\n",
    "    \"dropout\": [0, 0.5],\n",
    "    \"losses\": ['categorical_crossentropy'],\n",
    "    'kernel_initializer': ['uniform','normal'],\n",
    "    \"optimizer\": [Adam, SGD],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"epochs\": [100, 200, 500],\n",
    "    \"lr\": [0.005, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Originales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se entrena la *LSTM* para los datos a los que se le aplicó un preprocesado básico.\n",
    "\n",
    "Dividimos el dataset en entrenamiento, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nº train 7650.0000 Nº val 1350.0000 Nº test 1000.0000'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_orig, df_test_orig = train_test_split(df, test_size=0.10)\n",
    "df_train_orig, df_val_orig = train_test_split(df_train_orig, test_size=0.15)\n",
    "\n",
    "\"Nº train {0:.04f} Nº val {1:.04f} Nº test {2:.04f}\".format(len(df_train_orig), len(df_val_orig), len(df_test_orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos las etiquetas de *Score* a formato *one-hot encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_orig = df_train_orig.Text\n",
    "Ytrain_orig = df_train_orig.Score\n",
    "\n",
    "Xtest_orig = df_test_orig.Text\n",
    "Ytest_orig = df_test_orig.Score\n",
    "\n",
    "Xval_orig = df_val_orig.Text\n",
    "Yval_orig = df_val_orig.Score\n",
    "\n",
    "le = LabelEncoder()\n",
    "Ytrain_orig = le.fit_transform(Ytrain_orig)\n",
    "Ytest_orig = le.fit_transform(Ytest_orig)\n",
    "Yval_orig = le.fit_transform(Yval_orig)\n",
    "\n",
    "Ytrain_orig = Ytrain_orig.reshape(-1, 1)\n",
    "Ytest_orig = Ytest_orig.reshape(-1, 1)\n",
    "Yval_orig = Yval_orig.reshape(-1, 1)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "Ytrain_orig = enc.fit_transform(Ytrain_orig)\n",
    "Ytest_orig = enc.fit_transform(Ytest_orig)\n",
    "Yval_orig = enc.fit_transform(Yval_orig)\n",
    "\n",
    "Ytrain_orig = Ytrain_orig.toarray()\n",
    "Ytest_orig = Ytest_orig.toarray()\n",
    "Yval_orig = Yval_orig.toarray()\n",
    "\n",
    "Ytest_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos las opiniones a secuencias numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  22, 207, 901],\n",
       "       [  0,   0,   0, ..., 284, 150, 336],\n",
       "       [  0,   0,   0, ...,  45,   1, 193],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  42,   7,  74],\n",
       "       [  0,   0,   0, ...,  74,   7, 797],\n",
       "       [  0,   0,   0, ...,  96, 654, 148]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(num_words = max_words)\n",
    "\n",
    "tok.fit_on_texts(Xtrain_orig)\n",
    "\n",
    "train_orig = tok.texts_to_sequences(Xtrain_orig)\n",
    "train_orig_matrix = sequence.pad_sequences(train_orig, maxlen=max_len)\n",
    "\n",
    "test_orig = tok.texts_to_sequences(Xtest_orig)\n",
    "test_orig_matrix = sequence.pad_sequences(test_orig, maxlen=max_len)\n",
    "\n",
    "val_orig = tok.texts_to_sequences(Xval_orig)\n",
    "val_orig_matrix = sequence.pad_sequences(val_orig, maxlen=max_len)\n",
    "\n",
    "val_orig_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la optimización de hiperparámetros. Los resultados se guardan en el objeto *t* y en archivos *csv* en la carpeta *lstm_classifier_originales*. Pasamos por parámetro la función *modelo* que define a la red, y el diccionario de parámetros *parameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [22:11<00:00, 266.24s/it]\n"
     ]
    }
   ],
   "source": [
    "t = talos.Scan(x=train_orig_matrix,\n",
    "               y=Ytrain_orig,\n",
    "               x_val=val_orig_matrix,\n",
    "               y_val=Yval_orig,\n",
    "               model=modelo,\n",
    "               params=parameters,\n",
    "               experiment_name='lstm_classifier_originales',\n",
    "               round_limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vemos los resultados para cada una de las rondas del optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>fc1_dim</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>lstm_dim</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/09/20-163704</td>\n",
       "      <td>12/09/20-164143</td>\n",
       "      <td>278.874527</td>\n",
       "      <td>11</td>\n",
       "      <td>1.491676</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/20-164143</td>\n",
       "      <td>12/09/20-164626</td>\n",
       "      <td>282.664845</td>\n",
       "      <td>11</td>\n",
       "      <td>1.129904</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/20-164626</td>\n",
       "      <td>12/09/20-165112</td>\n",
       "      <td>285.730194</td>\n",
       "      <td>11</td>\n",
       "      <td>1.379455</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/09/20-165112</td>\n",
       "      <td>12/09/20-165400</td>\n",
       "      <td>168.395534</td>\n",
       "      <td>11</td>\n",
       "      <td>1.139502</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/09/20-165401</td>\n",
       "      <td>12/09/20-165915</td>\n",
       "      <td>314.275641</td>\n",
       "      <td>11</td>\n",
       "      <td>1.306240</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start              end    duration  round_epochs      loss  \\\n",
       "0  12/09/20-163704  12/09/20-164143  278.874527            11  1.491676   \n",
       "1  12/09/20-164143  12/09/20-164626  282.664845            11  1.129904   \n",
       "2  12/09/20-164626  12/09/20-165112  285.730194            11  1.379455   \n",
       "3  12/09/20-165112  12/09/20-165400  168.395534            11  1.139502   \n",
       "4  12/09/20-165401  12/09/20-165915  314.275641            11  1.306240   \n",
       "\n",
       "   accuracy  val_loss  val_accuracy  batch_size  dropout  emb_dim  epochs  \\\n",
       "0  0.637255       0.0           0.0          16      0.5       25     100   \n",
       "1  0.637255       0.0           0.0          16      0.5       25     500   \n",
       "2  0.637255       0.0           0.0          16      0.5       50     500   \n",
       "3  0.637255       0.0           0.0          32      0.5       50     100   \n",
       "4  0.637255       0.0           0.0          16      0.0       50     100   \n",
       "\n",
       "   fc1_dim kernel_initializer                    losses     lr  lstm_dim  \\\n",
       "0      256             normal  categorical_crossentropy  0.005       128   \n",
       "1      512            uniform  categorical_crossentropy  0.005       128   \n",
       "2      512            uniform  categorical_crossentropy  0.010       128   \n",
       "3      256             normal  categorical_crossentropy  0.005       128   \n",
       "4      128             normal  categorical_crossentropy  0.010       128   \n",
       "\n",
       "                                           optimizer  \n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "2  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "3  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "4  <class 'tensorflow.python.keras.optimizer_v2.g...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el objeto *Evaluate* podemos evaluar cada uno de los resultados del optimizador aplicando el conjunto de *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean : 0.16 \n",
      " std : 0.01\n"
     ]
    }
   ],
   "source": [
    "e = Evaluate(t)\n",
    "evaluation = e.evaluate(test_orig_matrix, \n",
    "                        Ytest_orig,\n",
    "                        folds=10,\n",
    "                        shuffle=False,\n",
    "                        metric='accuracy',\n",
    "                        print_out=True,\n",
    "                        task='multi_label',\n",
    "                        asc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente tabla se muestran los resultados de cada ronda de evaluación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>fc1_dim</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>lstm_dim</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/09/20-163704</td>\n",
       "      <td>12/09/20-164143</td>\n",
       "      <td>278.874527</td>\n",
       "      <td>11</td>\n",
       "      <td>1.491676</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/20-164143</td>\n",
       "      <td>12/09/20-164626</td>\n",
       "      <td>282.664845</td>\n",
       "      <td>11</td>\n",
       "      <td>1.129904</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/20-164626</td>\n",
       "      <td>12/09/20-165112</td>\n",
       "      <td>285.730194</td>\n",
       "      <td>11</td>\n",
       "      <td>1.379455</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/09/20-165112</td>\n",
       "      <td>12/09/20-165400</td>\n",
       "      <td>168.395534</td>\n",
       "      <td>11</td>\n",
       "      <td>1.139502</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/09/20-165401</td>\n",
       "      <td>12/09/20-165915</td>\n",
       "      <td>314.275641</td>\n",
       "      <td>11</td>\n",
       "      <td>1.306240</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start              end    duration  round_epochs      loss  \\\n",
       "0  12/09/20-163704  12/09/20-164143  278.874527            11  1.491676   \n",
       "1  12/09/20-164143  12/09/20-164626  282.664845            11  1.129904   \n",
       "2  12/09/20-164626  12/09/20-165112  285.730194            11  1.379455   \n",
       "3  12/09/20-165112  12/09/20-165400  168.395534            11  1.139502   \n",
       "4  12/09/20-165401  12/09/20-165915  314.275641            11  1.306240   \n",
       "\n",
       "   accuracy  val_loss  val_accuracy  batch_size  dropout  emb_dim  epochs  \\\n",
       "0  0.637255       0.0           0.0          16      0.5       25     100   \n",
       "1  0.637255       0.0           0.0          16      0.5       25     500   \n",
       "2  0.637255       0.0           0.0          16      0.5       50     500   \n",
       "3  0.637255       0.0           0.0          32      0.5       50     100   \n",
       "4  0.637255       0.0           0.0          16      0.0       50     100   \n",
       "\n",
       "   fc1_dim kernel_initializer                    losses     lr  lstm_dim  \\\n",
       "0      256             normal  categorical_crossentropy  0.005       128   \n",
       "1      512            uniform  categorical_crossentropy  0.005       128   \n",
       "2      512            uniform  categorical_crossentropy  0.010       128   \n",
       "3      256             normal  categorical_crossentropy  0.005       128   \n",
       "4      128             normal  categorical_crossentropy  0.010       128   \n",
       "\n",
       "                                           optimizer  \n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "2  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "3  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "4  <class 'tensorflow.python.keras.optimizer_v2.g...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Talos* nos permite extraer el mejor modelo encontrado con la función *best_model*. Utilizamos como métrica de comparación entre resultados la precisión.\n",
    "\n",
    "En el resumen devuelto vemos los mejores hiperparámetros para la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_orig = t.best_model(metric='accuracy', asc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 25)           25000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               78848     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 138,157\n",
      "Trainable params: 138,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_orig.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados\n",
    "\n",
    "Con los datos con un preprocesado básico se consigue un resultado de 63.72% de *accuracy* y un *loss* de 1.49, con los siguientes parámetros escogidos por *talos*:\n",
    "- Número de épocas realizadas: 11 de 100 épocas\n",
    "- Tamaño de batch: 16\n",
    "- Dimensión del *embedding*: 25\n",
    "- Dimensión del *lstm*: 128\n",
    "- Dimensión de la capa *fully-connected*: 256\n",
    "- Dropout: 0.5\n",
    "- Optimizador: SGD\n",
    "- Learning rate : 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Del mismo modo que en el apartado anterior, a continuación se entrena la *LSTM* para los datos a los que se le aplicó un preprocesado con *stemming*.\n",
    "\n",
    "Dividimos el dataset en entrenamiento, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nº train 7650.0000 Nº val 1350.0000 Nº test 1000.0000'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_stem, df_test_stem = train_test_split(df_stm, test_size=0.10)\n",
    "df_train_stem, df_val_stem = train_test_split(df_train_stem, test_size=0.15)\n",
    "\n",
    "\"Nº train {0:.04f} Nº val {1:.04f} Nº test {2:.04f}\".format(len(df_train_stem), len(df_val_stem), len(df_test_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierte las etiquetas de *Score* a formato *one-hot encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_stem = df_train_stem.Text\n",
    "Ytrain_stem = df_train_stem.Score\n",
    "\n",
    "Xtest_stem = df_test_stem.Text\n",
    "Ytest_stem = df_test_stem.Score\n",
    "\n",
    "Xval_stem = df_val_stem.Text\n",
    "Yval_stem = df_val_stem.Score\n",
    "\n",
    "le = LabelEncoder()\n",
    "Ytrain_stem = le.fit_transform(Ytrain_stem)\n",
    "Ytest_stem = le.fit_transform(Ytest_stem)\n",
    "Yval_stem = le.fit_transform(Yval_stem)\n",
    "\n",
    "Ytrain_stem = Ytrain_stem.reshape(-1, 1)\n",
    "Ytest_stem = Ytest_stem.reshape(-1, 1)\n",
    "Yval_stem = Yval_stem.reshape(-1, 1)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "Ytrain_stem = enc.fit_transform(Ytrain_stem)\n",
    "Ytest_stem = enc.fit_transform(Ytest_stem)\n",
    "Yval_stem = enc.fit_transform(Yval_stem)\n",
    "\n",
    "Ytrain_stem = Ytrain_stem.toarray()\n",
    "Ytest_stem = Ytest_stem.toarray()\n",
    "Yval_stem = Yval_stem.toarray()\n",
    "\n",
    "Ytest_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierte a secuencia numérica las opiniones, truncando a una longitud máxima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 316, 193,  48],\n",
       "       [  0,   0,   0, ..., 415, 308, 149],\n",
       "       [  0,   0,   0, ...,  54,  41, 187],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   3, 157,  91],\n",
       "       [  0,   0,   0, ...,  17, 192,  91],\n",
       "       [  0,   0,   0, ...,   3, 157,  91]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(num_words = max_words)\n",
    "\n",
    "tok.fit_on_texts(Xtrain_stem)\n",
    "\n",
    "train_stem = tok.texts_to_sequences(Xtrain_stem)\n",
    "train_stem_matrix = sequence.pad_sequences(train_stem, maxlen=max_len)\n",
    "\n",
    "test_stem = tok.texts_to_sequences(Xtest_stem)\n",
    "test_stem_matrix = sequence.pad_sequences(test_stem, maxlen=max_len)\n",
    "\n",
    "val_stem = tok.texts_to_sequences(Xval_stem)\n",
    "val_stem_matrix = sequence.pad_sequences(val_stem, maxlen=max_len)\n",
    "\n",
    "val_stem_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la optimización de hiperparámetros. En esta ocasión, los resultados se vuelcan en archivos *csv* en la carpeta *lstm_classifier_stemming*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [19:28<00:00, 233.78s/it]\n"
     ]
    }
   ],
   "source": [
    "t = talos.Scan(x=train_stem_matrix,\n",
    "               y=Ytrain_stem,\n",
    "               x_val=val_stem_matrix,\n",
    "               y_val=Yval_stem,\n",
    "               model=modelo,\n",
    "               params=parameters,\n",
    "               experiment_name='lstm_classifier_stemming',\n",
    "               round_limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente tabla se encuentran los resultados de cada ronda de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>fc1_dim</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>lstm_dim</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/09/20-171030</td>\n",
       "      <td>12/09/20-171234</td>\n",
       "      <td>124.035707</td>\n",
       "      <td>11</td>\n",
       "      <td>1.423369</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/20-171235</td>\n",
       "      <td>12/09/20-171738</td>\n",
       "      <td>303.127330</td>\n",
       "      <td>11</td>\n",
       "      <td>1.129079</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>256</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/20-171738</td>\n",
       "      <td>12/09/20-172249</td>\n",
       "      <td>310.539641</td>\n",
       "      <td>11</td>\n",
       "      <td>1.478115</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>256</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/09/20-172249</td>\n",
       "      <td>12/09/20-172751</td>\n",
       "      <td>302.406538</td>\n",
       "      <td>11</td>\n",
       "      <td>1.479346</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/09/20-172751</td>\n",
       "      <td>12/09/20-172959</td>\n",
       "      <td>127.544448</td>\n",
       "      <td>11</td>\n",
       "      <td>1.121236</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start              end    duration  round_epochs      loss  \\\n",
       "0  12/09/20-171030  12/09/20-171234  124.035707            11  1.423369   \n",
       "1  12/09/20-171235  12/09/20-171738  303.127330            11  1.129079   \n",
       "2  12/09/20-171738  12/09/20-172249  310.539641            11  1.478115   \n",
       "3  12/09/20-172249  12/09/20-172751  302.406538            11  1.479346   \n",
       "4  12/09/20-172751  12/09/20-172959  127.544448            11  1.121236   \n",
       "\n",
       "   accuracy  val_loss  val_accuracy  batch_size  dropout  emb_dim  epochs  \\\n",
       "0  0.635033       0.0           0.0          16      0.0       25     100   \n",
       "1  0.635033       0.0           0.0          16      0.0       25     500   \n",
       "2  0.635033       0.0           0.0          16      0.5       50     500   \n",
       "3  0.635033       0.0           0.0          16      0.5       50     100   \n",
       "4  0.635033       0.0           0.0          16      0.0       50     500   \n",
       "\n",
       "   fc1_dim kernel_initializer                    losses     lr  lstm_dim  \\\n",
       "0      256             normal  categorical_crossentropy  0.010        32   \n",
       "1      256            uniform  categorical_crossentropy  0.005       128   \n",
       "2      256            uniform  categorical_crossentropy  0.005       128   \n",
       "3      128            uniform  categorical_crossentropy  0.005       128   \n",
       "4      512            uniform  categorical_crossentropy  0.005        32   \n",
       "\n",
       "                                           optimizer  \n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "2  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "3  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "4  <class 'tensorflow.python.keras.optimizer_v2.a...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando los datos de test evaluamos a cada uno de los resultados de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean : 0.16 \n",
      " std : 0.02\n"
     ]
    }
   ],
   "source": [
    "e = Evaluate(t)\n",
    "evaluation = e.evaluate(test_stem_matrix, \n",
    "                        Ytest_stem,\n",
    "                        folds=10,\n",
    "                        shuffle=False,\n",
    "                        metric='accuracy',\n",
    "                        print_out=True,\n",
    "                        task='multi_label',\n",
    "                        asc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra cada uno de los resultados de la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>fc1_dim</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>lstm_dim</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/09/20-171030</td>\n",
       "      <td>12/09/20-171234</td>\n",
       "      <td>124.035707</td>\n",
       "      <td>11</td>\n",
       "      <td>1.423369</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/20-171235</td>\n",
       "      <td>12/09/20-171738</td>\n",
       "      <td>303.127330</td>\n",
       "      <td>11</td>\n",
       "      <td>1.129079</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>256</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/20-171738</td>\n",
       "      <td>12/09/20-172249</td>\n",
       "      <td>310.539641</td>\n",
       "      <td>11</td>\n",
       "      <td>1.478115</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>256</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/09/20-172249</td>\n",
       "      <td>12/09/20-172751</td>\n",
       "      <td>302.406538</td>\n",
       "      <td>11</td>\n",
       "      <td>1.479346</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/09/20-172751</td>\n",
       "      <td>12/09/20-172959</td>\n",
       "      <td>127.544448</td>\n",
       "      <td>11</td>\n",
       "      <td>1.121236</td>\n",
       "      <td>0.635033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start              end    duration  round_epochs      loss  \\\n",
       "0  12/09/20-171030  12/09/20-171234  124.035707            11  1.423369   \n",
       "1  12/09/20-171235  12/09/20-171738  303.127330            11  1.129079   \n",
       "2  12/09/20-171738  12/09/20-172249  310.539641            11  1.478115   \n",
       "3  12/09/20-172249  12/09/20-172751  302.406538            11  1.479346   \n",
       "4  12/09/20-172751  12/09/20-172959  127.544448            11  1.121236   \n",
       "\n",
       "   accuracy  val_loss  val_accuracy  batch_size  dropout  emb_dim  epochs  \\\n",
       "0  0.635033       0.0           0.0          16      0.0       25     100   \n",
       "1  0.635033       0.0           0.0          16      0.0       25     500   \n",
       "2  0.635033       0.0           0.0          16      0.5       50     500   \n",
       "3  0.635033       0.0           0.0          16      0.5       50     100   \n",
       "4  0.635033       0.0           0.0          16      0.0       50     500   \n",
       "\n",
       "   fc1_dim kernel_initializer                    losses     lr  lstm_dim  \\\n",
       "0      256             normal  categorical_crossentropy  0.010        32   \n",
       "1      256            uniform  categorical_crossentropy  0.005       128   \n",
       "2      256            uniform  categorical_crossentropy  0.005       128   \n",
       "3      128            uniform  categorical_crossentropy  0.005       128   \n",
       "4      512            uniform  categorical_crossentropy  0.005        32   \n",
       "\n",
       "                                           optimizer  \n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "2  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "3  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "4  <class 'tensorflow.python.keras.optimizer_v2.a...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos el mejor modelo y observamos los parámetros escogidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 25)           25000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                7424      \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 42,157\n",
      "Trainable params: 42,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_stem = t.best_model(metric='accuracy', asc=True)\n",
    "best_model_stem.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados\n",
    "\n",
    "Con los datos preprocesados con *stemming* se consigue un resultado de 63.5% de *accuracy* y un *loss* de 1.42, con los siguientes parámetros escogidos por *talos*:\n",
    "- Número de épocas realizadas: 11 de 100 épocas\n",
    "- Tamaño de batch: 16\n",
    "- Dimensión del *embedding*: 25\n",
    "- Dimensión del *lstm*: 32\n",
    "- Dimensión de la capa *fully-connected*: 256\n",
    "- Dropout: 0.0\n",
    "- Optimizador: SGD\n",
    "- Learning rate : 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos a continuación el mismo procedimiento para entrenar la *LSTM* para los datos a los que se le aplicó un preprocesado con *lemmatization*.\n",
    "\n",
    "Dividimos el dataset en entrenamiento, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nº train 7650.0000 Nº val 1350.0000 Nº test 1000.0000'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_lem, df_test_lem = train_test_split(df_lem, test_size=0.10)\n",
    "df_train_lem, df_val_lem = train_test_split(df_train_lem, test_size=0.15)\n",
    "\n",
    "\"Nº train {0:.04f} Nº val {1:.04f} Nº test {2:.04f}\".format(len(df_train_lem), len(df_val_lem), len(df_test_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificamos la etiqueta *Score* a *one-hot encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_lem = df_train_lem.Text\n",
    "Ytrain_lem = df_train_lem.Score\n",
    "\n",
    "Xtest_lem = df_test_lem.Text\n",
    "Ytest_lem = df_test_lem.Score\n",
    "\n",
    "Xval_lem = df_val_lem.Text\n",
    "Yval_lem = df_val_lem.Score\n",
    "\n",
    "le = LabelEncoder()\n",
    "Ytrain_lem = le.fit_transform(Ytrain_lem)\n",
    "Ytest_lem = le.fit_transform(Ytest_lem)\n",
    "Yval_lem = le.fit_transform(Yval_lem)\n",
    "\n",
    "Ytrain_lem = Ytrain_lem.reshape(-1, 1)\n",
    "Ytest_lem = Ytest_lem.reshape(-1, 1)\n",
    "Yval_lem = Yval_lem.reshape(-1, 1)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "Ytrain_lem = enc.fit_transform(Ytrain_lem)\n",
    "Ytest_lem = enc.fit_transform(Ytest_lem)\n",
    "Yval_lem = enc.fit_transform(Yval_lem)\n",
    "\n",
    "Ytrain_lem = Ytrain_lem.toarray()\n",
    "Ytest_lem = Ytest_lem.toarray()\n",
    "Yval_lem = Yval_lem.toarray()\n",
    "\n",
    "Ytest_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos a secuencia numérica las opiniones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  24,  13,  35],\n",
       "       [  0,   0,   0, ..., 137,   4, 457],\n",
       "       [  0,   0,   0, ..., 891, 400, 755],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  22,  93, 164],\n",
       "       [  0,   0,   0, ...,  84,  12, 784],\n",
       "       [  0,   0,   0, ..., 800,  74, 186]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer(num_words = max_words)\n",
    "\n",
    "tok.fit_on_texts(Xtrain_lem)\n",
    "\n",
    "train_lem = tok.texts_to_sequences(Xtrain_lem)\n",
    "train_lem_matrix = sequence.pad_sequences(train_lem, maxlen=max_len)\n",
    "\n",
    "test_lem = tok.texts_to_sequences(Xtest_lem)\n",
    "test_lem_matrix = sequence.pad_sequences(test_lem, maxlen=max_len)\n",
    "\n",
    "val_lem = tok.texts_to_sequences(Xval_lem)\n",
    "val_lem_matrix = sequence.pad_sequences(val_lem, maxlen=max_len)\n",
    "\n",
    "val_lem_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la optimización de hiperparámetros. Los resultados los encontraremos en el directorio *lstm_classifier_lemmatization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [12:25<00:00, 149.11s/it]\n"
     ]
    }
   ],
   "source": [
    "t = talos.Scan(x=train_lem_matrix,\n",
    "               y=Ytrain_lem,\n",
    "               x_val=val_lem_matrix,\n",
    "               y_val=Yval_lem,\n",
    "               model=modelo,\n",
    "               params=parameters,\n",
    "               experiment_name='lstm_classifier_lemmatization',\n",
    "               round_limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente tabla están los resultados para cada solución obtenida por el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>fc1_dim</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>lstm_dim</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/09/20-181833</td>\n",
       "      <td>12/09/20-182014</td>\n",
       "      <td>100.941295</td>\n",
       "      <td>11</td>\n",
       "      <td>1.130054</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/20-182014</td>\n",
       "      <td>12/09/20-182248</td>\n",
       "      <td>153.817730</td>\n",
       "      <td>11</td>\n",
       "      <td>1.495309</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>256</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/20-182248</td>\n",
       "      <td>12/09/20-182745</td>\n",
       "      <td>296.177344</td>\n",
       "      <td>11</td>\n",
       "      <td>1.391446</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/09/20-182745</td>\n",
       "      <td>12/09/20-182949</td>\n",
       "      <td>124.104380</td>\n",
       "      <td>11</td>\n",
       "      <td>1.122573</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/09/20-182949</td>\n",
       "      <td>12/09/20-183058</td>\n",
       "      <td>69.017961</td>\n",
       "      <td>11</td>\n",
       "      <td>1.503583</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>512</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start              end    duration  round_epochs      loss  \\\n",
       "0  12/09/20-181833  12/09/20-182014  100.941295            11  1.130054   \n",
       "1  12/09/20-182014  12/09/20-182248  153.817730            11  1.495309   \n",
       "2  12/09/20-182248  12/09/20-182745  296.177344            11  1.391446   \n",
       "3  12/09/20-182745  12/09/20-182949  124.104380            11  1.122573   \n",
       "4  12/09/20-182949  12/09/20-183058   69.017961            11  1.503583   \n",
       "\n",
       "   accuracy  val_loss  val_accuracy  batch_size  dropout  emb_dim  epochs  \\\n",
       "0  0.637516       0.0           0.0          32      0.5       25     500   \n",
       "1  0.637516       0.0           0.0          16      0.5       25     500   \n",
       "2  0.637516       0.0           0.0          16      0.0       25     500   \n",
       "3  0.637516       0.0           0.0          16      0.0       25     100   \n",
       "4  0.637516       0.0           0.0          32      0.5       50     200   \n",
       "\n",
       "   fc1_dim kernel_initializer                    losses     lr  lstm_dim  \\\n",
       "0      512             normal  categorical_crossentropy  0.010        64   \n",
       "1      256            uniform  categorical_crossentropy  0.005        64   \n",
       "2      512            uniform  categorical_crossentropy  0.010       128   \n",
       "3      128             normal  categorical_crossentropy  0.005        32   \n",
       "4      512             normal  categorical_crossentropy  0.010        32   \n",
       "\n",
       "                                           optimizer  \n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "2  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "3  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "4  <class 'tensorflow.python.keras.optimizer_v2.g...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos cada solución mediante el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean : 0.16 \n",
      " std : 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>fc1_dim</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>losses</th>\n",
       "      <th>lr</th>\n",
       "      <th>lstm_dim</th>\n",
       "      <th>optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/09/20-181833</td>\n",
       "      <td>12/09/20-182014</td>\n",
       "      <td>100.941295</td>\n",
       "      <td>11</td>\n",
       "      <td>1.130054</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/20-182014</td>\n",
       "      <td>12/09/20-182248</td>\n",
       "      <td>153.817730</td>\n",
       "      <td>11</td>\n",
       "      <td>1.495309</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>256</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/09/20-182248</td>\n",
       "      <td>12/09/20-182745</td>\n",
       "      <td>296.177344</td>\n",
       "      <td>11</td>\n",
       "      <td>1.391446</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>512</td>\n",
       "      <td>uniform</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/09/20-182745</td>\n",
       "      <td>12/09/20-182949</td>\n",
       "      <td>124.104380</td>\n",
       "      <td>11</td>\n",
       "      <td>1.122573</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.005</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/09/20-182949</td>\n",
       "      <td>12/09/20-183058</td>\n",
       "      <td>69.017961</td>\n",
       "      <td>11</td>\n",
       "      <td>1.503583</td>\n",
       "      <td>0.637516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>512</td>\n",
       "      <td>normal</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'tensorflow.python.keras.optimizer_v2.g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start              end    duration  round_epochs      loss  \\\n",
       "0  12/09/20-181833  12/09/20-182014  100.941295            11  1.130054   \n",
       "1  12/09/20-182014  12/09/20-182248  153.817730            11  1.495309   \n",
       "2  12/09/20-182248  12/09/20-182745  296.177344            11  1.391446   \n",
       "3  12/09/20-182745  12/09/20-182949  124.104380            11  1.122573   \n",
       "4  12/09/20-182949  12/09/20-183058   69.017961            11  1.503583   \n",
       "\n",
       "   accuracy  val_loss  val_accuracy  batch_size  dropout  emb_dim  epochs  \\\n",
       "0  0.637516       0.0           0.0          32      0.5       25     500   \n",
       "1  0.637516       0.0           0.0          16      0.5       25     500   \n",
       "2  0.637516       0.0           0.0          16      0.0       25     500   \n",
       "3  0.637516       0.0           0.0          16      0.0       25     100   \n",
       "4  0.637516       0.0           0.0          32      0.5       50     200   \n",
       "\n",
       "   fc1_dim kernel_initializer                    losses     lr  lstm_dim  \\\n",
       "0      512             normal  categorical_crossentropy  0.010        64   \n",
       "1      256            uniform  categorical_crossentropy  0.005        64   \n",
       "2      512            uniform  categorical_crossentropy  0.010       128   \n",
       "3      128             normal  categorical_crossentropy  0.005        32   \n",
       "4      512             normal  categorical_crossentropy  0.010        32   \n",
       "\n",
       "                                           optimizer  \n",
       "0  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "1  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "2  <class 'tensorflow.python.keras.optimizer_v2.g...  \n",
       "3  <class 'tensorflow.python.keras.optimizer_v2.a...  \n",
       "4  <class 'tensorflow.python.keras.optimizer_v2.g...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = Evaluate(t)\n",
    "evaluation = e.evaluate(test_lem_matrix, \n",
    "                        Ytest_lem,\n",
    "                        folds=10,\n",
    "                        shuffle=False,\n",
    "                        metric='accuracy',\n",
    "                        print_out=True,\n",
    "                        task='multi_label',\n",
    "                        asc=False)\n",
    "e.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el mejor modelo y su configuración de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 150)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 25)           25000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                23040     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 83,885\n",
      "Trainable params: 83,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model_lem = t.best_model(metric='accuracy', asc=True)\n",
    "best_model_lem.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados\n",
    "\n",
    "Con los datos preprocesados con *lemmatization* se consigue un resultado de 63.75% de *accuracy* y un *loss* de 1.13, con los siguientes parámetros escogidos por *talos*:\n",
    "- Número de épocas realizadas: 11 de 100 épocas\n",
    "- Tamaño de batch: 32\n",
    "- Dimensión del *embedding*: 25\n",
    "- Dimensión del *lstm*: 64\n",
    "- Dimensión de la capa *fully-connected*: 512\n",
    "- Dropout: 0.5\n",
    "- Optimizador: Adam\n",
    "- Learning rate : 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Según los resultados obtenidos no parece que el preprocesado varíe notablemente el resultado de precisión de la red. Aún así, la configuración de la red *LSTM* debe ser mayor cuando se utilizan datos sin preprocesar mediante *stemming* o *lemmatization*. La red óptima que utilizó datos no preprocesados es de 128 dimensiones y tardó ~278 segundos en entrenar, en contraste a las que utilizaron datos preprocesados, que tienen una dimensión de 64 y tardaron ~100 segundos en entrenar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
